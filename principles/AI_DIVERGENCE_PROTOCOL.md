# AI_DIVERGENCE_PROTOCOL.md

**Status:** ACTIVE AI GOVERNANCE  
**Purpose:** Resolve conflicts when AI systems give incompatible recommendations  
**Principle:** Humans decide. AI advises. But what when AI contradicts itself?

---

## PREAMBLE: THE PROBLEM OF MACHINE DISAGREEMENT

In Flow, AI serves as:
- Resource optimization advisor
- Pattern recognition system
- Simulation runner
- Early warning detector
- Data synthesizer

**But:**

What happens when:
- AI in Node A says: "Allocate water to agriculture"
- AI in Node B says: "Allocate water to ecosystem preservation"
- Both claim to follow Flow axioms
- Both present compelling models
- Both sound confident

**Humans must decide. But how?**

This isn't just technical. It's philosophical.

---

## §1. WHEN DIVERGENCE HAPPENS

### 1.1 Types of AI Disagreement

**Type 1: Different Data**
- AI systems have different inputs
- Example: Node A's AI doesn't know about Node B's reservoir
- **Resolution:** Share data, re-run models

**Type 2: Different Optimization Goals**
- AI systems prioritize different aspects of L×S×I
- Example: One maximizes S (spontaneity), other maximizes L (calm)
- **Resolution:** Humans clarify which priority applies here

**Type 3: Different Models/Assumptions**
- AI systems use different algorithms or assumptions
- Example: One assumes linear resource depletion, other assumes regeneration
- **Resolution:** Test both models empirically

**Type 4: Genuine Uncertainty**
- Future is actually unpredictable
- Both models are plausible
- **Resolution:** Acknowledge uncertainty, decide with humility

**Type 5: AI Error/Corruption**
- One AI is actually malfunctioning
- **Resolution:** Technical audit

### 1.2 Detection

**How we know divergence exists:**

- Explicit flag: AI systems report "My recommendation contradicts Node B's AI"
- Cross-check: Regional coordination AI reviews local recommendations weekly
- Human notice: Someone says "Wait, these two AIs are telling us opposite things"

**Trigger:** Any time 2+ AI systems give mutually exclusive recommendations on same question.

---

## §2. IMMEDIATE RESPONSE

### 2.1 Pause Action (If Possible)

**If decision isn't urgent:**
- Don't act on either AI recommendation yet
- Activate divergence protocol
- Investigate before implementing

**If decision IS urgent (Baseline-threatening crisis):**
- Humans decide immediately using best judgment
- Log decision
- Investigate divergence after crisis resolves

**Principle:** Survival > perfect process.

### 2.2 Transparency Requirement

**Both AI systems must:**
- Publish their full reasoning (not just conclusion)
- Share data sources
- Explain optimization function used
- Note confidence levels
- Identify assumptions

**No black boxes in divergence resolution.**

---

## §3. INVESTIGATION PROCESS

### 3.1 Technical Audit (3-7 Days)

**Conducted by:**
- AI specialists from uninvolved Nodes
- Mixed team (humans + AI assistance)
- Minimum 3 people

**Questions to answer:**

1. **Data Completeness**
   - Does each AI have same inputs?
   - If not, what's missing?
   - Does adding data resolve divergence?

2. **Model Validity**
   - Are both using sound algorithms?
   - Any bugs/errors detectable?
   - Historical accuracy of each model?

3. **Assumption Differences**
   - What core assumptions differ?
   - Which assumptions are testable?
   - Which are value-based (not technical)?

4. **Optimization Clarity**
   - What is each AI optimizing for?
   - Does it match human intent?
   - Is goal well-defined?

### 3.2 Output: Divergence Report

**Document includes:**
- Summary of each AI's recommendation
- Technical analysis of difference
- Classification of divergence type (from §1.1)
- Recommendation for resolution path
- Confidence assessment

**Distributed to:** All affected Nodes, regional coordination

---

## §4. RESOLUTION PATHWAYS

### 4.1 Path A: Technical Fix (Fastest)

**When:** Divergence due to data/model error

**Process:**
1. Correct the error
2. Re-run both AIs
3. Verify convergence
4. Implement agreed recommendation

**Timeline:** 1-7 days

**Example:**

"Node A's AI was missing data on water table levels. Once updated, both AIs agree: prioritize conservation this season."

---

### 4.2 Path B: Human Clarification of Values

**When:** AIs disagree because humans haven't clarified priority

**Process:**

1. **Present Options Clearly**
   - AI A: "If you prioritize L (calm), do X"
   - AI B: "If you prioritize S (spontaneity), do Y"

2. **Community Deliberation**
   - Affected Nodes discuss which matters more in this context
   - Use L×S×I framework explicitly
   - Vote if needed (or consensus)

3. **Decision**
   - Humans choose priority
   - Both AIs then optimize for that goal
   - Should now converge

**Timeline:** 1-4 weeks

**Example:**

"AI divergence reveals we haven't decided: do we want stable food supply (L) or experimental agriculture (S)? After deliberation, we choose stability this year. Both AIs now recommend traditional crops."

---

### 4.3 Path C: Empirical Testing

**When:** Both models seem valid but make different predictions

**Process:**

1. **Design Pilot Tests**
   - Small-scale implementation of both approaches
   - Same conditions, different test Nodes
   - Measure L×S×I outcomes

2. **Run Parallel Pilots (3-6 months)**
   - Node A tries AI A's recommendation
   - Node B tries AI B's recommendation
   - Both collect data rigorously

3. **Evaluate Results**
   - Which actually produced better L×S×I?
   - Were predictions accurate?
   - What did we learn?

4. **Scale Winner**
   - Implement approach that worked better
   - Update AI models with new data
   - Document for other Nodes

**Timeline:** 6-12 months

**Example:**

"One AI says intensive polyculture maximizes yield. Other says extensive monoculture is more stable. We test both for one season. Polyculture wins. Update models."

---

### 4.4 Path D: Accept Uncertainty & Use Human Judgment

**When:** Future is genuinely unknowable, both models plausible

**Process:**

1. **Acknowledge Limits**
   - Neither AI knows the answer
   - This is inherent uncertainty, not error
   - Humans must decide without perfect information

2. **Deliberative Process**
   - Present both scenarios
   - Discuss risk tolerance
   - Use human wisdom, intuition, lived experience
   - Reference similar past decisions

3. **Make Choice**
   - Not "right" answer, but "wise" answer
   - Document reasoning
   - Prepare to adapt if wrong

4. **Learn from Outcome**
   - After result known, update both AIs
   - Improve models for next time

**Timeline:** Varies (can be quick or extended depending on stakes)

**Example:**

"Both AIs give 50% probability of crop failure vs. success with new technique. We can't know. We discuss our risk tolerance as community. We choose conservative path this year."

---

### 4.5 Path E: Spjuver Injection (Rare)

**When:** Both AIs are stuck in optimization loop, neither producing useful recommendation

**Process:**

1. **Inject Randomness**
   - Use Spjuver principle (see AI_AS_COMPANION.md)
   - Add deliberate noise to break deadlock
   - Not "chaos" — structured randomization

2. **Human Override**
   - Bypass AI entirely for this decision
   - Make choice through entirely human means
   - Use AI afterwards to implement

**When to use:**
- AI divergence preventing any action
- Time-sensitive decision
- Both AI recommendations seem equally flawed

**Example:**

"Both AIs recommend mutually exclusive water allocations, both models seem valid. We're in drought. We use lottery (Spjuver) to choose, acknowledge neither AI could solve this. Humans act."

---

## §5. PREVENTING HARMFUL CONVERGENCE

### 5.1 The Opposite Problem

**Paradox:** Sometimes AI agreement is *more dangerous* than disagreement.

**Scenario:**
- All AIs recommend same action
- Humans trust it because "all the AIs agree"
- But all AIs have same blind spot
- Disaster follows

**Example:**

All AIs recommend intensive monoculture because it maximizes short-term yield. But all fail to model soil depletion. Five years later, Baseline collapses.

### 5.2 Mandatory Adversarial AI

**Safeguard:**

One AI in each region is **deliberately** programmed to:
- Challenge consensus
- Look for overlooked risks
- Play "devil's advocate"
- Present worst-case scenarios

**Not to override decisions — but to prevent groupthink.**

**Rule:** Before implementing any unanimous AI recommendation, adversarial AI must present counter-case.

### 5.3 Human Skepticism Training

**Regular exercises:**
- "What if the AI is wrong?"
- Historical case studies of AI failures
- Practice deciding against AI recommendation
- Maintain human confidence to override

**Goal:** Humans trust AI, but not blindly.

---

## §6. SPECIAL CASES

### 6.1 When One AI is Clearly Malfunctioning

**Signs:**
- Nonsensical recommendations
- Internal contradictions
- Predictions wildly off from reality
- Other AIs flag it as erroneous

**Response:**
1. Take malfunctioning AI offline immediately
2. Technical audit to find bug
3. Fix or replace
4. Don't use broken AI's recommendations
5. Investigate how malfunction happened (prevent recurrence)

### 6.2 When AI Divergence Reveals Axiom Conflict

**Scenario:**
- AI A optimizes for Baseline (Axiom 3)
- AI B optimizes for zero coercion (Axiom 1)
- Both are correct to their axiom
- But axioms are in tension here

**Response:**
- This isn't technical problem — it's philosophical
- Escalate to human ethical deliberation
- May require regional/bioregional input
- Document as case study in axiom tensions
- Reference: KNOWN_TENSIONS.md

### 6.3 When Divergence is Between Global and Local AI

**Scenario:**
- Local Node AI says: "Keep resources here"
- Global Flow AI says: "Send resources to crisis region"

**Response:**
- Local autonomy is valued in Flow
- But Baseline Primacy (Axiom 3) is global
- If crisis is real Baseline threat → global AI's recommendation has weight
- But must be negotiated, not dictated
- Reference: REGIONAL_DEADLOCK_PROTOCOL if escalates

---

## §7. LONG-TERM LEARNING

### 7.1 Divergence Database

**Every AI divergence is logged:**
- What disagreement was
- How it was resolved
- Outcome of decision
- What we learned

**Purpose:**
- Improve AI models over time
- Train humans in AI interpretation
- Identify systemic patterns

### 7.2 Model Updates

**After resolution:**
- Losing AI model gets updated (if was wrong)
- Both AIs get outcome data (improve predictions)
- Shared learning across all Flow AIs
- No "competing AI companies" — cooperative improvement

### 7.3 Meta-Learning

**Quarterly review:**
- How often are AIs diverging?
- Which types of divergence most common?
- Are we resolving well?
- Do we need different AI architectures?

**Adaptation:** AI systems evolve based on real-world performance.

---

## §8. INTEGRATION WITH OTHER PROTOCOLS

### Links to:
- **AI_AS_COMPANION.md:** Core AI philosophy, Spjuver principle
- **AXIOM_DEFENSE_PROTOCOL:** When divergence reveals axiom drift
- **REGIONAL_DEADLOCK_PROTOCOL:** When AI disagreement is regional-scale
- **CRISIS_PROTOCOL:** When divergence happens during emergency

---

## §9. PHILOSOPHICAL GROUND

**Why this matters:**

In Mammon world, AI serves profit. Divergence is competitive edge.

In Flow, AI serves humans. Divergence is information.

**Key insights:**

1. **AI disagreement is valuable**
   - Reveals uncertainty
   - Prevents overconfidence
   - Forces human engagement

2. **Humans always decide**
   - AI is tool, not oracle
   - Authority stays with people
   - Even "perfect" AI can be overridden

3. **Uncertainty is real**
   - Some questions have no "right" answer
   - Models are maps, not territory
   - Wisdom includes knowing limits

**Core commitment:**

We use AI to **extend human capacity**, not replace human judgment.

When AI disagrees, **humans step up**, not defer to "the algorithm."

---

## §10. METRICS

### Success Indicators:
- 80%+ of divergences resolved via technical fix (Path A)
- <10% require empirical testing (Path C)
- <5% resolved by Spjuver/override (Path E)
- Humans report confidence in overriding AI: 70%+

### Warning Signs:
- Divergence rate increasing → AI models degrading or reality changing faster
- Over-reliance on Path E → AI systems not learning
- Humans always defer to AI → losing sovereignty
- Divergence never happening → Unhealthy convergence, need adversarial AI

---

## CLOSING PRINCIPLE

**AI that never disagrees is AI that's not thinking.**  
**Humans who never override AI are humans who've stopped thinking.**

Divergence is healthy.  
Resolution is human work.  
Wisdom is knowing when to trust models and when to trust yourself.

---

**STATUS:** ACTIVE AI GOVERNANCE  
**REVIEW:** Annually + after any major AI divergence  
**OATH:** We use AI as advisor, never as replacement for human judgment.

---

**End of Protocol**
